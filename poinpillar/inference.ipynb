{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pointpillars import PointPillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from model.pointpillars import PointPillars\n",
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fully connected layer\n",
    "class FCModel(nn.Module):\n",
    "      def __init__(self, original_model):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.pointpillars = original_model\n",
    "        self.fc = nn.Linear(256 * 62 * 54, 512)  # Fully connected layer to reduce to 512 dimensions\n",
    "\n",
    "      def forward(self, x):\n",
    "          # Get features from the PointPillars backbone\n",
    "          features = self.pointpillars(x)  # Assuming output shape is [B, 256, 62, 54]\n",
    "          \n",
    "          if isinstance(features, list):\n",
    "            features = features[-1]  # Use the last feature tensor if multiple outputs exist\n",
    "          # Flatten features\n",
    "          \n",
    "          features_flat = features.view(features.size(0), -1)  # Flatten to [B, 256 * 62 * 54]\n",
    "          \n",
    "          # Pass through the FC layer\n",
    "          reduced_features = self.fc(features_flat)  # Output shape is [B, 512]\n",
    "          return reduced_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCModel(\n",
       "  (pointpillars): PointPillars(\n",
       "    (pillar_layer): PillarLayer(\n",
       "      (voxel_layer): Voxelization(\n",
       "        (voxelizer): HardVoxelization()\n",
       "      )\n",
       "    )\n",
       "    (pillar_encoder): PillarEncoder(\n",
       "      (conv): Conv1d(9, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (backbone): Backbone(\n",
       "      (multi_blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=857088, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `model` is the PointPillars model\n",
    "pretrained_model_path = r\"C:\\Users\\hussa\\OneDrive\\Desktop\\Projects\\ROS2-Modular-Framework-for-End-to-End-Autonomous-Vehicle-Control-from-Raw-Sensor-Data\\poinpillar\\pre_trained\\epoch_160.pth\"\n",
    "model_state_dict = torch.load(pretrained_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Initialize the PointPillars model\n",
    "model = PointPillars(\n",
    "    nclasses=3,\n",
    "    voxel_size=[0.16, 0.16, 4],\n",
    "    point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "    max_num_points=32,\n",
    "    max_voxels=(16000, 40000)  # Training and inference max_voxels\n",
    ")\n",
    "\n",
    "# Load the filtered state dict into the model (without the head weights)\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "# # Now replace the head layer with Identity\n",
    "\n",
    "model.neck = nn.Identity()\n",
    "model.head = nn.Identity()\n",
    "\n",
    "model = FCModel(model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess PCD file\n",
    "def load_pcd_file(pcd_path):\n",
    "    # Load the point cloud data using Open3D\n",
    "    pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "\n",
    "    # Convert points to numpy array\n",
    "    points = np.asarray(pcd.points)  # (N, 3)\n",
    "\n",
    "    # Check if colors are available and add them\n",
    "    if pcd.has_colors():\n",
    "        colors = np.asarray(pcd.colors)  # (N, 3)\n",
    "        data = np.hstack((points, colors))  # Combine points and colors\n",
    "    else:\n",
    "        data = points  # Only points available\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Preprocess the point cloud data for the model\n",
    "def preprocess_point_cloud(pcd_points):\n",
    "\n",
    "    # For example, voxel size (you may need to tune this)\n",
    "    voxel_size = 0.2\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcd_points)\n",
    "    downsampled_pcd = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    # Convert the downsampled points to numpy array\n",
    "    downsampled_points = np.asarray(downsampled_pcd.points)  # Shape: (N', 3)\n",
    "\n",
    "    # Convert to torch tensor and add batch dimension\n",
    "    input_tensor = torch.tensor(downsampled_points, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    return input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def run_inference(input_tensor):\n",
    "    with torch.no_grad():\n",
    "        # Run the model inference (assuming the model returns features directly)\n",
    "        features = model(input_tensor)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-153.81289672851562 121.6452865600586 -66.90157318115234 96.09734344482422\n",
      "275.4581832885742 162.99891662597656\n",
      "torch.Size([1, 512])\n",
      "fea tensor([[-2.4810,  0.8540, -0.4522, -0.3311,  0.8102, -0.0539, -0.3551, -1.9374,\n",
      "          0.0669,  0.4492, -0.4861, -1.0186,  0.5423, -0.5881, -1.2853, -1.0903,\n",
      "         -0.9193, -0.6869, -0.3095, -0.1703, -1.3830,  0.7142,  0.1527, -0.0593,\n",
      "          2.0985,  1.8965,  0.0577,  0.3120, -0.5745, -2.0948,  1.4861, -1.2225,\n",
      "          0.9504, -0.6405,  0.9987, -0.2844,  0.5083,  0.7006,  0.0780,  0.3686,\n",
      "         -0.6501, -0.0199, -0.6352,  0.7376, -1.1079, -1.2099, -0.9091,  0.8616,\n",
      "         -0.1441, -1.2520, -0.0346, -0.1112, -1.0732, -0.0260,  1.2822,  0.6861,\n",
      "         -0.3417,  0.9892, -0.9693,  0.8202, -0.9028, -0.8242, -0.4553, -1.3038,\n",
      "          1.0806,  0.6932,  0.3632,  0.6487,  0.4570,  0.4616,  0.6220,  0.7195,\n",
      "          0.8832, -0.6363, -0.8894,  0.5026,  1.3928,  0.1092,  0.3461,  0.3314,\n",
      "         -0.5511,  0.3914, -0.6886,  0.3008,  0.0445,  0.5150, -1.1632,  0.5883,\n",
      "          0.6989, -0.0689, -1.9953, -1.1280,  0.4960,  1.5925, -0.4146, -0.6636,\n",
      "          1.4203, -0.5373,  1.8653,  0.8850, -2.6315,  0.5885,  1.0186, -0.1145,\n",
      "         -0.2144,  0.6983, -1.8535, -0.6138,  1.8662, -1.7563,  0.6022, -0.0922,\n",
      "         -2.1172, -1.0938, -1.8972,  1.3563,  0.5510,  0.1375, -0.5452,  0.1912,\n",
      "         -0.0228, -0.6416,  0.6768,  2.7738,  1.0849,  1.2508,  0.1483,  3.3073,\n",
      "         -0.9337, -0.9559, -0.1360, -0.1397, -0.0107,  1.1636, -0.2053,  0.4218,\n",
      "          0.0963, -1.7163, -0.2265,  0.4703, -2.5039, -0.6571, -0.5447, -0.0478,\n",
      "          0.2566,  0.9230, -0.1267, -0.1659,  0.8798,  1.8451,  0.0072,  0.3800,\n",
      "         -0.6504, -1.5532, -2.4788,  0.5201, -0.3406, -1.6745, -0.7103,  0.4166,\n",
      "         -0.1828,  0.9866, -1.2771, -0.0664, -1.6627,  0.2906,  0.5320,  0.0043,\n",
      "         -0.6968, -1.2669, -1.2836, -0.3366, -1.5014, -0.8339, -1.1074, -1.8452,\n",
      "          0.1518,  0.8095, -0.0552,  0.1626, -0.5288, -0.4666, -0.9747,  0.1577,\n",
      "         -0.1527, -1.6227,  0.7248,  0.8723, -0.7470, -1.5921, -0.8421, -1.2165,\n",
      "         -0.8461,  2.0496, -0.3992, -0.2818, -1.8032, -1.0716, -0.4413, -0.2958,\n",
      "          3.1771, -1.2869,  0.0557, -0.0243, -0.1982,  0.3770, -0.1326,  0.2875,\n",
      "         -0.8911,  0.6648, -1.3196, -0.3521,  1.5205, -1.4892, -1.2860,  1.2947,\n",
      "          0.5246,  0.8307, -0.3319, -0.2109,  0.3403, -0.5287, -1.5688, -3.0067,\n",
      "          0.5039,  0.3577,  0.5883,  0.4962,  1.2352,  0.3256, -1.0322, -1.2120,\n",
      "         -2.3042, -0.5445, -0.4299, -0.7985,  0.8653,  0.5751,  1.0113,  0.7857,\n",
      "         -0.0751,  0.1098,  1.0833, -0.9214,  0.2482, -0.5656,  0.2581, -0.0140,\n",
      "          0.0389, -1.0490,  2.7813,  0.1941, -1.6832, -1.1830, -0.3736, -0.5346,\n",
      "          2.6112, -0.2705,  0.6466,  0.5254,  0.5351,  0.5978,  0.3414,  1.0970,\n",
      "          0.5102, -1.5379, -0.8670, -0.2946,  0.7654, -1.3049,  0.3331,  3.6790,\n",
      "          0.6722,  1.5492,  0.5502, -0.6056,  0.8637, -0.4413,  1.4523,  0.4023,\n",
      "          0.2864,  0.1549, -0.1770,  1.4763,  0.5353,  1.7052, -0.7866,  1.5237,\n",
      "          1.3499,  0.1234,  0.2568,  0.0457, -0.2208, -2.2773, -1.4722, -0.8872,\n",
      "          1.9590,  1.3581,  1.0425,  1.7800, -0.5544,  0.2244,  1.3741,  0.3238,\n",
      "          2.3876, -1.0417,  0.1720, -1.4129, -0.2101,  0.4449,  0.1998, -0.9998,\n",
      "         -0.4198,  0.5485,  0.8181, -0.1026, -0.1378, -0.2082, -0.9959,  0.2073,\n",
      "          0.4603,  0.2686, -1.0021,  0.6420, -0.0921, -1.3556,  1.7895, -0.8216,\n",
      "          1.3412, -0.7430,  0.7434,  1.2988,  1.9905,  0.1979, -0.8698, -1.7859,\n",
      "         -0.4874,  2.3928, -0.0780, -0.3300,  0.9974, -0.0240, -0.2867, -1.9531,\n",
      "          0.6071,  0.1923,  0.3969,  0.2803,  0.7296,  1.7351, -0.4490, -2.3203,\n",
      "          1.4359,  0.4201,  0.3416, -1.0212, -0.9978, -0.3991, -1.3541,  0.3924,\n",
      "         -0.9150, -1.2710, -0.1941,  0.5848, -0.6852, -0.1273,  0.0950, -1.0382,\n",
      "         -0.1708, -0.7267,  0.4452, -1.5842,  1.0491, -0.1223, -0.5254,  0.4850,\n",
      "         -0.8503, -0.5055,  0.9358,  0.9288, -1.1012, -1.2943, -1.2984,  1.0484,\n",
      "          0.7569, -0.5616,  0.5381,  1.1900, -1.8914,  2.7549, -0.7542, -0.4854,\n",
      "         -0.0927, -0.6505,  1.5189,  0.6992, -1.8455,  0.4464, -2.1617,  0.0732,\n",
      "          0.7678, -0.8570, -0.2034, -0.7320, -0.0316,  1.4162, -1.3697,  0.0206,\n",
      "         -1.1478, -1.2563, -0.0981,  0.3841,  1.3301, -1.2854, -0.3453,  0.3124,\n",
      "          1.3198,  0.8752, -0.7685,  1.5424,  1.0590,  1.5443,  0.6680,  0.2013,\n",
      "         -1.1815,  0.8557,  0.2297, -0.0624, -0.0894, -1.9439,  0.5893,  2.6884,\n",
      "          0.3910, -1.3050,  0.0301, -0.1245,  0.3276,  0.7058,  0.8189,  2.6793,\n",
      "          0.5928,  0.3707,  0.5282,  1.3304, -0.2187, -0.6362,  0.4424,  1.1758,\n",
      "          1.1329,  1.2676,  0.4147, -0.7636,  0.3383, -0.3585, -1.1647,  0.7426,\n",
      "         -1.1810,  0.9996, -0.8308,  0.5611, -0.5359, -1.7261, -0.7922, -1.3709,\n",
      "          1.6093, -1.1289, -0.9759,  0.1287, -0.5621,  0.1971,  1.0894,  1.2085,\n",
      "         -0.1209,  1.3912,  0.0471,  0.0194,  1.9408,  0.8404,  1.8851, -0.7559,\n",
      "         -0.5863,  0.7858,  0.8233, -2.3679,  1.7826, -0.4829, -0.5074,  0.3967,\n",
      "          1.7579, -0.3340,  2.8248, -0.1361, -0.7637, -0.7217,  0.2953, -1.0582,\n",
      "          2.5181, -0.2369,  0.4129, -0.7357,  1.0281, -0.8249, -1.3174, -0.0748,\n",
      "         -0.6769,  0.4879,  0.0240, -0.1874, -1.7985,  0.0079, -0.0636,  0.5864]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pcd_path = r\"C:\\Users\\hussa\\OneDrive\\Desktop\\Projects\\Project\\MOT\\INFRA-3DRC_scene-15\\INFRA-3DRC_scene-15\\lidar_01\\lidar_01__data\\lidar_01__2023-06-02-21-28-09-321.pcd\"\n",
    "points = load_pcd_file(pcd_path)\n",
    "\n",
    "x_min, x_max = np.min(points[:, 0]), np.max(points[:, 0])\n",
    "y_min, y_max = np.min(points[:, 1]), np.max(points[:, 1])\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "\n",
    "print(x_range, y_range)\n",
    "\n",
    "# Preprocess the point cloud data\n",
    "input_tensor = preprocess_point_cloud(points)\n",
    "\n",
    "# Run inference\n",
    "features = run_inference(input_tensor)\n",
    "\n",
    "print(features.shape)\n",
    "print(f'fea {features}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
