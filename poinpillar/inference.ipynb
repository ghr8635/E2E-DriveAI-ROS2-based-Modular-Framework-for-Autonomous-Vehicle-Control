{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pointpillars import PointPillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from model.pointpillars import PointPillars\n",
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fully connected layer\n",
    "class FCModel(nn.Module):\n",
    "      def __init__(self, original_model):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.pointpillars = original_model\n",
    "        self.fc = nn.Linear(256 * 62 * 54, 512)  # Fully connected layer to reduce to 512 dimensions\n",
    "\n",
    "      def forward(self, x):\n",
    "          # Get features from the PointPillars backbone\n",
    "          features = self.pointpillars(x)  # Assuming output shape is [B, 256, 62, 54]\n",
    "          \n",
    "          if isinstance(features, list):\n",
    "            features = features[-1]  # Use the last feature tensor if multiple outputs exist\n",
    "          # Flatten features\n",
    "          \n",
    "          features_flat = features.view(features.size(0), -1)  # Flatten to [B, 256 * 62 * 54]\n",
    "          \n",
    "          # Pass through the FC layer\n",
    "          reduced_features = self.fc(features_flat)  # Output shape is [B, 512]\n",
    "          return reduced_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCModel(\n",
       "  (pointpillars): PointPillars(\n",
       "    (pillar_layer): PillarLayer(\n",
       "      (voxel_layer): Voxelization(\n",
       "        (voxelizer): HardVoxelization()\n",
       "      )\n",
       "    )\n",
       "    (pillar_encoder): PillarEncoder(\n",
       "      (conv): Conv1d(9, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (backbone): Backbone(\n",
       "      (multi_blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=857088, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `model` is the PointPillars model\n",
    "pretrained_model_path = r\"C:\\Users\\hussa\\OneDrive\\Desktop\\Projects\\ROS2-Modular-Framework-for-End-to-End-Autonomous-Vehicle-Control-from-Raw-Sensor-Data\\poinpillar\\pre_trained\\epoch_160.pth\"\n",
    "model_state_dict = torch.load(pretrained_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Initialize the PointPillars model\n",
    "model = PointPillars(\n",
    "    nclasses=3,\n",
    "    voxel_size=[0.16, 0.16, 4],\n",
    "    point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "    max_num_points=32,\n",
    "    max_voxels=(16000, 40000)  # Training and inference max_voxels\n",
    ")\n",
    "\n",
    "# Load the filtered state dict into the model (without the head weights)\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "\n",
    "# # Now replace the head layer with Identity\n",
    "\n",
    "model.neck = nn.Identity()\n",
    "model.head = nn.Identity()\n",
    "\n",
    "model = FCModel(model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess PCD file\n",
    "def load_pcd_file(pcd_path):\n",
    "    # Load the point cloud data using Open3D\n",
    "    pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "\n",
    "    # Convert points to numpy array\n",
    "    points = np.asarray(pcd.points)  # (N, 3)\n",
    "\n",
    "    # Check if colors are available and add them\n",
    "    if pcd.has_colors():\n",
    "        colors = np.asarray(pcd.colors)  # (N, 3)\n",
    "        data = np.hstack((points, colors))  # Combine points and colors\n",
    "    else:\n",
    "        data = points  # Only points available\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Preprocess the point cloud data for the model\n",
    "def preprocess_point_cloud(pcd_points):\n",
    "\n",
    "    # For example, voxel size (you may need to tune this)\n",
    "    voxel_size = 0.2\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcd_points)\n",
    "    downsampled_pcd = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    # Convert the downsampled points to numpy array\n",
    "    downsampled_points = np.asarray(downsampled_pcd.points)  # Shape: (N', 3)\n",
    "\n",
    "    # Convert to torch tensor and add batch dimension\n",
    "    input_tensor = torch.tensor(downsampled_points, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    return input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def run_inference(input_tensor):\n",
    "    with torch.no_grad():\n",
    "        # Run the model inference (assuming the model returns features directly)\n",
    "        features = model(input_tensor)\n",
    "        features = features [0]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-153.81289672851562 121.6452865600586 -66.90157318115234 96.09734344482422\n",
      "275.4581832885742 162.99891662597656\n",
      "torch.Size([512])\n",
      "fea tensor([-1.8531, -0.3836, -0.4546, -0.4256, -0.1361, -2.0734,  0.4105,  0.4851,\n",
      "        -1.6765,  0.8794, -0.7291,  0.7760, -1.5365,  0.1842, -0.6723, -0.2770,\n",
      "         1.0689,  0.3012, -2.1551, -0.1950,  1.7721, -0.2861, -0.0331,  0.2265,\n",
      "        -0.2852,  0.4287, -0.6591, -1.1174,  0.6874,  0.6386, -0.5327,  0.8086,\n",
      "        -0.4387, -0.4726, -1.4557, -0.6632,  1.3620, -0.4663, -0.7752, -0.2972,\n",
      "         0.1262,  0.8827, -0.0695,  2.6200,  0.4138, -0.5700, -1.1610, -0.6748,\n",
      "        -1.1426, -0.7170, -0.1808, -1.2347,  0.2934, -1.6884,  0.7570, -0.5544,\n",
      "         0.7118, -1.7226,  0.5021, -0.4243,  0.6524, -1.5342,  1.3904,  0.0579,\n",
      "         0.0794, -0.7581, -0.7264, -0.8413, -0.1647, -0.1311, -1.9294, -0.6574,\n",
      "         0.9973, -1.7068, -0.9051,  0.4159, -0.1744, -2.1463,  0.3225, -0.1520,\n",
      "         0.1628, -0.6362, -0.4997,  0.9595,  0.3888,  0.8242, -0.5442, -1.4957,\n",
      "         1.2567,  1.0570, -0.7687,  0.2952, -1.9113, -1.2468, -0.2848, -0.7809,\n",
      "         1.5463, -0.3756,  0.1940,  0.7698, -1.5675, -0.2094,  0.6631,  1.5796,\n",
      "         0.9441, -0.4451,  0.5727, -0.7354, -0.7347,  1.7111,  1.3578, -0.3650,\n",
      "        -0.5622,  0.3119,  1.2699, -0.0068,  0.6514,  1.2135, -0.5203, -0.8124,\n",
      "         0.3938,  0.5009,  0.0212, -0.9377, -0.4366,  0.1643, -0.1288, -2.0134,\n",
      "        -0.3981, -0.0446,  0.5445, -0.0796,  0.1640,  0.5521,  1.2106,  1.6707,\n",
      "         3.1761,  1.2927, -0.6470, -0.2415,  0.5704,  2.1142,  1.0017, -1.1894,\n",
      "        -0.9045, -0.3674, -0.0401, -0.5868, -0.8897, -1.2776, -2.2766, -0.5177,\n",
      "        -0.2692,  0.1551,  0.1893,  1.2803,  0.2217,  1.4579, -0.4428, -0.1264,\n",
      "         2.1431, -0.3797, -1.1326,  0.0703,  0.8917, -0.7059,  0.0702,  2.0289,\n",
      "        -0.7660, -0.4392, -1.2811,  2.1928,  0.3358,  0.4520, -0.4156,  0.8668,\n",
      "         1.9719,  0.3014,  0.8755, -0.4801,  0.7685,  0.1749, -0.2443, -0.0270,\n",
      "         0.2358, -0.2026, -0.1068,  2.3542,  0.5341, -0.5067,  0.4093, -1.2429,\n",
      "        -0.1115,  0.2828, -0.8670,  0.0674, -0.0219,  0.4359, -0.7574,  0.3399,\n",
      "        -0.2929,  1.5974,  0.1048, -1.3852,  0.2908,  0.1718, -0.3941,  0.5484,\n",
      "        -0.3871,  1.7249,  0.3687, -0.5798, -0.9395, -1.9974, -1.0950,  1.2639,\n",
      "         2.7759,  0.9873,  3.0694,  0.3471, -0.4299, -1.0055,  0.7435,  0.5177,\n",
      "         2.1130,  1.0137, -1.0341,  0.5418, -0.1508, -0.4053,  1.0914,  0.3848,\n",
      "        -0.3124, -1.7517, -1.1527, -0.0175,  0.6895, -0.9595, -0.9250, -1.5440,\n",
      "         0.6617,  0.6130, -0.6590,  0.1872, -0.5413, -0.0580,  0.4176,  1.7363,\n",
      "         0.9155, -0.2159, -1.1563, -0.3713,  1.4055,  2.3844,  0.7337, -0.7916,\n",
      "        -0.8072,  0.1901,  0.0884,  0.2784, -0.1772, -0.7404,  0.1344,  0.2999,\n",
      "         0.8938, -1.1541,  1.0560,  0.2889, -0.9255,  0.5472,  1.1868,  0.2879,\n",
      "        -0.9436,  0.1649,  0.1561, -1.5780,  0.1694,  2.2836,  0.3523, -1.1180,\n",
      "        -0.3982, -0.9222,  0.4322, -0.0301,  0.2428, -0.1651,  0.4376, -0.2427,\n",
      "        -1.0790, -0.4018, -1.1389, -1.5097, -1.3340,  1.7817,  0.6826,  1.8659,\n",
      "        -1.3099, -0.1427,  0.9047, -1.7448, -1.2965,  0.8277,  1.4594, -0.5895,\n",
      "         0.8829,  0.6927, -0.8678,  0.2295, -0.0426, -0.1685,  0.1584, -0.2262,\n",
      "        -0.0446,  0.3052,  0.4012,  0.6953,  1.5450, -0.4034,  0.5545,  0.6170,\n",
      "        -0.9847,  0.0436, -0.1782, -0.1581, -2.7262, -0.1894,  0.2019,  0.3315,\n",
      "        -1.4605, -0.6130,  1.0782,  0.2600,  0.1918, -1.7355,  0.9898,  0.2440,\n",
      "         0.5695,  1.6005,  0.5861, -0.9421, -2.3145,  0.1437,  2.4048,  0.0823,\n",
      "        -1.8065, -0.6066,  0.8025,  0.2848, -0.7587,  1.0573, -1.2954, -1.7292,\n",
      "         0.3505, -1.3068, -1.9471,  0.4870, -0.5688, -1.1798, -0.2067,  0.1080,\n",
      "         1.8885,  1.0629, -1.2932, -0.6125, -0.0555, -0.5087, -1.3119, -2.0739,\n",
      "         1.7865,  2.3627,  0.1138,  0.1263, -1.3669,  1.2538, -1.7795,  0.2831,\n",
      "        -0.1656,  0.2912,  0.4129, -0.4485,  0.0999, -2.5825, -2.1783, -1.2524,\n",
      "        -0.4050,  3.1213, -1.3046, -0.6445, -0.1238,  0.3306, -1.0069, -0.0185,\n",
      "         0.3824,  0.5218, -0.5031, -1.3343,  1.6121, -0.9680, -0.0914, -0.3042,\n",
      "        -0.5301,  1.9839,  0.1462,  0.7461, -2.0994, -0.2222, -1.0720, -0.1270,\n",
      "         0.2880, -0.6785, -0.7115,  1.2509,  0.1502, -0.7138, -1.9440,  1.1872,\n",
      "         0.2904, -0.5059,  0.6108, -0.4053,  1.7703,  1.4600, -1.1707, -0.1994,\n",
      "         0.8262, -1.3739, -1.2292,  0.0598, -0.7656, -0.4348, -1.3588, -1.0677,\n",
      "         0.5567,  0.5698,  0.2361, -0.8172,  0.7187,  0.2448, -0.5349, -0.4216,\n",
      "         1.1937,  1.0935, -0.5718, -1.7608, -0.6355, -1.2929, -2.3321,  0.3377,\n",
      "        -0.7677, -0.7907,  2.7258, -1.1445, -0.3174,  0.0369,  0.8215, -0.2132,\n",
      "         2.1219, -0.0807,  0.0317,  0.5448,  0.5448,  0.3465,  0.1089,  0.5607,\n",
      "        -1.0151, -0.3159, -2.5237, -0.2175, -0.3398, -0.4499,  0.1466,  0.9625,\n",
      "        -1.3953,  0.0658,  0.3858, -0.1584, -1.0888, -0.7955,  0.8111,  2.1483,\n",
      "         1.6077, -1.5860,  1.0361, -0.6830,  0.9641,  1.0654,  1.9280,  0.9146,\n",
      "         1.7305, -1.0446,  0.6267, -0.9504, -0.5519, -0.8680,  0.9918, -0.9868,\n",
      "        -0.7534,  1.8112, -0.4831, -0.4945, -0.6516, -1.1883,  0.2794, -0.6545,\n",
      "         1.6888, -1.3294,  1.2373,  0.6666,  1.9878, -0.0914, -0.0099,  1.6905])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pcd_path = r\"C:\\Users\\hussa\\OneDrive\\Desktop\\Projects\\Project\\MOT\\INFRA-3DRC_scene-15\\INFRA-3DRC_scene-15\\lidar_01\\lidar_01__data\\lidar_01__2023-06-02-21-28-09-321.pcd\"\n",
    "points = load_pcd_file(pcd_path)\n",
    "\n",
    "x_min, x_max = np.min(points[:, 0]), np.max(points[:, 0])\n",
    "y_min, y_max = np.min(points[:, 1]), np.max(points[:, 1])\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "\n",
    "print(x_range, y_range)\n",
    "\n",
    "# Preprocess the point cloud data\n",
    "input_tensor = preprocess_point_cloud(points)\n",
    "\n",
    "# Run inference\n",
    "features = run_inference(input_tensor)\n",
    " \n",
    "print(features.shape)\n",
    "print(f'fea {features}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
