{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyg/qmMQlSXlT29sSS6W99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghr8635/E2E-DriveAI-ROS2-based-Modular-Framework-for-Autonomous-Vehicle-Control/blob/main/self_built_point_pillar_architecture_(simple_data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCGmgan5fsR5",
        "outputId": "4de5f80e-7179-4977-f9cf-7c37859afb30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ntN1VvWWwgKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the PointPillars Feature Extraction Model**"
      ],
      "metadata": {
        "id": "_lY8g0e4w1q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PillarFeatureEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_points_per_pillar, grid_x, grid_y):\n",
        "        super(PillarFeatureEncoder, self).__init__()\n",
        "        self.num_points_per_pillar = num_points_per_pillar\n",
        "        self.grid_x, self.grid_y = grid_x, grid_y\n",
        "        # Simple linear layer to encode pillar features\n",
        "        self.fc = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, pillars):\n",
        "        # Flatten and encode features\n",
        "        pillars = self.fc(pillars)  # Shape: (batch, grid_x, grid_y, num_points, out_channels)\n",
        "        return pillars.mean(dim=2)  # Reduce across point dimension to get (batch, grid_x, grid_y, out_channels)\n",
        "\n",
        "class BackboneNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BackboneNetwork, self).__init__()\n",
        "        # Simple 2D CNN layers for feature extraction\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)  # Shape: (batch, out_channels, grid_x//4, grid_y//4)\n",
        "        return x\n",
        "\n",
        "class PointPillarsFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels=4, out_channels=256, num_points_per_pillar=32, grid_x=100, grid_y=100):\n",
        "        super(PointPillarsFeatureExtractor, self).__init__()\n",
        "        self.pfe = PillarFeatureEncoder(in_channels, out_channels, num_points_per_pillar, grid_x, grid_y)\n",
        "        self.backbone = BackboneNetwork(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pillar_features = self.pfe(x)  # Shape: (batch, grid_x, grid_y, out_channels)\n",
        "        pillar_features = pillar_features.permute(0, 3, 1, 2)  # Shape: (batch, out_channels, grid_x, grid_y)\n",
        "        feature_map = self.backbone(pillar_features)\n",
        "        return feature_map"
      ],
      "metadata": {
        "id": "swqyoGh-wtHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare a Simple Dataset for Training**"
      ],
      "metadata": {
        "id": "ZeAKahNPw791"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticPointCloudDataset(Dataset):\n",
        "    def __init__(self, num_samples, grid_x=100, grid_y=100, num_points_per_pillar=32, in_channels=4):\n",
        "        self.num_samples = num_samples\n",
        "        self.grid_x, self.grid_y = grid_x, grid_y\n",
        "        self.num_points_per_pillar = num_points_per_pillar\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate random pillars with shape: (grid_x, grid_y, num_points, in_channels)\n",
        "        pillars = np.random.rand(self.grid_x, self.grid_y, self.num_points_per_pillar, self.in_channels).astype(np.float32)\n",
        "        return torch.tensor(pillars)\n",
        "\n",
        "# Parameters\n",
        "num_samples = 100\n",
        "dataset = SyntheticPointCloudDataset(num_samples)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Bx6dTsUsw99t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop (Optional, to learn feature extraction patterns)**"
      ],
      "metadata": {
        "id": "hbWR08KDxGN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model, optimizer, and criterion\n",
        "model = PointPillarsFeatureExtractor()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Simple Training Loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, pillars in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        features = model(pillars)  # Extracted features, shape: (batch, out_channels, grid_x//4, grid_y//4)\n",
        "\n",
        "        # Dummy target: here we're using the output itself as target for demonstration\n",
        "        target = features.clone().detach()  # This is just for illustrative purposes\n",
        "        loss = criterion(features, target)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C4D0rE1xHyR",
        "outputId": "b14f56cd-e3d0-4601-879e-37d78f67d9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [2/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [3/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [4/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [5/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [6/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [7/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [8/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [9/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [10/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [11/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [12/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [13/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [14/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [15/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [16/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [17/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [18/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [19/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [20/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [21/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [22/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [23/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [24/25], Loss: 0.0000\n",
            "Epoch [1/5], Batch [25/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [1/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [2/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [3/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [4/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [5/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [6/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [7/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [8/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [9/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [10/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [11/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [12/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [13/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [14/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [15/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [16/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [17/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [18/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [19/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [20/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [21/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [22/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [23/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [24/25], Loss: 0.0000\n",
            "Epoch [2/5], Batch [25/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [1/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [2/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [3/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [4/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [5/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [6/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [7/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [8/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [9/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [10/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [11/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [12/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [13/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [14/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [15/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [16/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [17/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [18/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [19/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [20/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [21/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [22/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [23/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [24/25], Loss: 0.0000\n",
            "Epoch [3/5], Batch [25/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [1/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [2/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [3/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [4/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [5/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [6/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [7/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [8/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [9/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [10/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [11/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [12/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [13/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [14/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [15/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [16/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [17/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [18/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [19/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [20/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [21/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [22/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [23/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [24/25], Loss: 0.0000\n",
            "Epoch [4/5], Batch [25/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [1/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [2/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [3/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [4/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [5/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [6/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [7/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [8/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [9/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [10/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [11/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [12/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [13/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [14/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [15/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [16/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [17/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [18/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [19/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [20/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [21/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [22/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [23/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [24/25], Loss: 0.0000\n",
            "Epoch [5/5], Batch [25/25], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving Model**"
      ],
      "metadata": {
        "id": "4NicKX-QxQG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path where you want to save the model\n",
        "model_save_path = \"/content/drive/MyDrive/ROS2-Modular-Framework-for-End-to-End-Autonomous-Vehicle-Control-from-Raw-Sensor-Data/self_built_point_pillar.pth\"\n",
        "\n",
        "# After training is complete, save the model\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl-zBVNjxPwp",
        "outputId": "6aa33f76-4eae-4bbf-ecfd-813c3ce2b279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/ROS2-Modular-Framework-for-End-to-End-Autonomous-Vehicle-Control-from-Raw-Sensor-Data/self_built_point_pillar.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Model for Inference**"
      ],
      "metadata": {
        "id": "o3sLSQCsxtRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model architecture\n",
        "loaded_model = PointPillarsFeatureExtractor()\n",
        "\n",
        "# Load the saved weights\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "# Set the model to evaluation mode if you are not training it further\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPgWXi7lxwFU",
        "outputId": "90dce578-508f-4c12-b140-f20bebc153d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2d3a27d5251c>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_model.load_state_dict(torch.load(model_save_path))\n"
          ]
        }
      ]
    }
  ]
}